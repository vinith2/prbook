
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction &#8212; Pattern Recognition</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"vect": ["\\boldsymbol{#1}", 1], "mat": ["\\boldsymbol{#1}", 1], "rvar": ["\\mathsf{#1}", 1], "rvect": ["\\mathsf{\\boldsymbol{#1}}", 1], "rmat": ["\\mathsf{\\boldsymbol{#1}}", 1], "vx": ["\\vect{x}"], "vy": ["\\vect{y}"], "vz": ["\\vect{z}"], "vv": ["\\vect{v}"], "vw": ["\\vect{w}"], "va": ["\\vect{a}"], "vb": ["\\vect{b}"], "vc": ["\\vect{c}"], "vd": ["\\vect{d}"], "ve": ["\\vect{e}"], "vf": ["\\vect{f}"], "vg": ["\\vect{g}"], "vh": ["\\vect{h}"], "vk": ["\\vect{k}"], "vp": ["\\vect{p}"], "vn": ["\\vect{n}"], "vell": ["\\vect{\\ell}"], "vmu": ["\\vect{\\mu}"], "mX": ["\\mat{X}"], "mY": ["\\mat{Y}"], "mZ": ["\\mat{Z}"], "mV": ["\\mat{V}"], "mW": ["\\mat{W}"], "mA": ["\\mat{A}"], "mB": ["\\mat{B}"], "mC": ["\\mat{C}"], "mD": ["\\mat{D}"], "mE": ["\\mat{E}"], "mF": ["\\mat{F}"], "mG": ["\\mat{G}"], "mH": ["\\mat{H}"], "mK": ["\\mat{K}"], "mP": ["\\mat{P}"], "mSigma": ["\\mat{\\Sigma}"], "mI": ["\\mat{I}"], "rx": ["\\rvar{x}"], "ry": ["\\rvar{y}"], "rz": ["\\rvar{z}"], "rv": ["\\rvar{v}"], "rw": ["\\rvar{w}"], "ra": ["\\rvar{a}"], "rb": ["\\rvar{b}"], "rc": ["\\rvar{c}"], "rd": ["\\rvar{d}"], "re": ["\\rvar{e}"], "rf": ["\\rvar{f}"], "rg": ["\\rvar{g}"], "rh": ["\\rvar{h}"], "rk": ["\\rvar{k}"], "rp": ["\\rvar{p}"], "rX": ["\\rvar{X}"], "rH": ["\\rvar{H}"], "rY": ["\\rvar{Y}"], "rvx": ["\\rvect{x}"], "rvy": ["\\rvect{y}"], "rvz": ["\\rvect{z}"], "rvv": ["\\rvect{v}"], "rvw": ["\\rvect{w}"], "rva": ["\\rvect{a}"], "rvb": ["\\rvect{b}"], "rvc": ["\\rvect{c}"], "rvd": ["\\rvect{d}"], "rve": ["\\rvect{e}"], "rvf": ["\\rvect{f}"], "rvg": ["\\rvect{g}"], "rvh": ["\\rvect{h}"], "rvk": ["\\rvect{k}"], "rvp": ["\\rvect{p}"], "rmX": ["\\rmat{X}"], "rmY": ["\\rmat{Y}"], "rmZ": ["\\rmat{Z}"], "rmV": ["\\rmat{V}"], "rmW": ["\\rmat{W}"], "rmA": ["\\rmat{A}"], "rmB": ["\\rmat{B}"], "rmC": ["\\rmat{C}"], "rmD": ["\\rmat{D}"], "rmE": ["\\rmat{E}"], "rmF": ["\\rmat{F}"], "rmG": ["\\rmat{G}"], "rmH": ["\\rmat{H}"], "rmK": ["\\rmat{K}"], "rmP": ["\\rmat{P}"], "EE": ["\\mathbb{E}"], "RR": ["\\mathbb{R}"], "CC": ["\\mathbb{C}"], "ZZ": ["\\mathbb{Z}"], "SS": ["\\mathbb{S}"], "norm": ["\\|#1\\|", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Modeling Knowledge" href="modeling-knowledge.html" />
    <link rel="prev" title="Installation" href="installation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo_sada-lab_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Pattern Recognition</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="modeling-knowledge.html">
   Modeling Knowledge
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="filtering.html">
   Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="denoise-deblur.html">
   Denoising and deblurring
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintroduction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/introduction.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="_sources/introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-the-plan">
   What is the plan?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-mnist-dataset">
   The MNIST dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mnist">
   MNIST
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#our-goal-ship-a-pr-system">
   Our goal: ship a PR system
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-focus-on-two-digits-binary-classification">
   Let’s focus on two digits: binary classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rule-0-simple-ideas-first">
   Rule 0: simple ideas first
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-the-right-weights">
   Choosing the right weights
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-make-this-into-an-algorithm">
   Let’s make this into an algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-do-the-weights-do">
   What do the weights do?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-different-classifier-counting-neighbor-labels">
   A different classifier: counting neighbor labels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-k-works-best">
   Which
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   works best?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-iris-flower-datset">
   The Iris flower datset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-perceptron">
   The perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#well-we-first-reinvented-the-perceptron">
   Well, we first reinvented the
   <em>
    perceptron
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-then-reinvented-k-nearest-neighbors">
   We then reinvented
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   nearest neighbors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-of-the-questions-we-must-answer">
   Some of the questions we must answer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-have-been-committing-a-crime">
   We have been committing a crime
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#this-course">
   This course
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beware">
   Beware
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-mathematical-tools">
   Key mathematical tools
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-theory-and-statistics">
     Probability theory (and statistics)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-algebra">
     Linear algebra
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-the-plan">
   What is the plan?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-mnist-dataset">
   The MNIST dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mnist">
   MNIST
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#our-goal-ship-a-pr-system">
   Our goal: ship a PR system
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-focus-on-two-digits-binary-classification">
   Let’s focus on two digits: binary classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rule-0-simple-ideas-first">
   Rule 0: simple ideas first
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-the-right-weights">
   Choosing the right weights
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#let-s-make-this-into-an-algorithm">
   Let’s make this into an algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-do-the-weights-do">
   What do the weights do?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-different-classifier-counting-neighbor-labels">
   A different classifier: counting neighbor labels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-k-works-best">
   Which
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   works best?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-iris-flower-datset">
   The Iris flower datset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-perceptron">
   The perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#well-we-first-reinvented-the-perceptron">
   Well, we first reinvented the
   <em>
    perceptron
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-then-reinvented-k-nearest-neighbors">
   We then reinvented
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   nearest neighbors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-of-the-questions-we-must-answer">
   Some of the questions we must answer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-have-been-committing-a-crime">
   We have been committing a crime
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#this-course">
   This course
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beware">
   Beware
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-mathematical-tools">
   Key mathematical tools
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-theory-and-statistics">
     Probability theory (and statistics)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-algebra">
     Linear algebra
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from tqdm import tqdm

mpl.rcParams[&#39;axes.spines.top&#39;] = 0
mpl.rcParams[&#39;axes.spines.right&#39;] = 0
mpl.rcParams[&#39;axes.spines.left&#39;] = 1
mpl.rcParams[&#39;axes.spines.bottom&#39;] = 1
mpl.rcParams.update({&#39;font.size&#39;: 12})
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.spines.top&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.spines.right&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;tqdm&#39;
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h1>
<section id="what-is-the-plan">
<h2>What is the plan?<a class="headerlink" href="#what-is-the-plan" title="Permalink to this headline">#</a></h2>
<p>Pattern recognition is about recognizing patterns. It is almost synonymous with supervised learning, although unsupervised learning plays an important role in feature design. To really understand pattern recognition you need to get comfortable with a certain amount of math, perhaps more than you would like. This will be hard unless you believe in it and perhaps even like it.</p>
<p>In order to create context for all that math, instead of starting by history or philosophy or by defining “patterns” and “recognitions”, let us start by writing some code to recognize patterns.</p>
</section>
<section id="the-mnist-dataset">
<h2>The MNIST dataset<a class="headerlink" href="#the-mnist-dataset" title="Permalink to this headline">#</a></h2>
<p>We will work with a notorious dataset of handwritten digits called MNIST. According to this interview, even Geoff Hinton, another godfather of deep learning and a Turing co-awardee of LeCun, tests ideas in Matlab on a subsampled MNIST dataset he can keep on his laptop and work with fast.</p>
<figure class="align-default" id="id1">
<img alt="_images/lecun-mnist.jpg" src="_images/lecun-mnist.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Yann LeCun and his MNIST dataset of handwritten digits.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Yann LeCun is a very early evangelist of convolutional neural networks trained by backpropagation. The role of datasets, challenges, benchmarks in the “deep learning revolution” was recognized already 20-something years ago by David Donoho in his essay. Their critical importance deserves a critical analytic eye, so machine learning researchers like Ben Recht are now looking at them. Yann LeCun is the creator of the indesctructible MNIST dataset (factcheck).</p>
</section>
<section id="mnist">
<h2>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline">#</a></h2>
<p>We start by loading and displaying a couple of digits. There are many, many ways to get MNIST digits in your Python environment and I simply chose one that looked reasonable for my purpose.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from mlxtend.data import loadlocal_mnist

X_train, y_train = loadlocal_mnist(
        images_path=&#39;./book_data/train-images-idx3-ubyte&#39;, 
        labels_path=&#39;./book_data/train-labels-idx1-ubyte&#39;
        )

fig, axs = plt.subplots(ncols=4, nrows=1, figsize=(14, 3))
for ax in axs:
    ax.imshow(X_train[np.random.randint(1000)].reshape(28, 28), cmap=&#39;gray&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="our-goal-ship-a-pr-system">
<h2>Our goal: ship a PR system<a class="headerlink" href="#our-goal-ship-a-pr-system" title="Permalink to this headline">#</a></h2>
<p>We are running ahead here so don’t worry if not every detail is super-clear to you at this point and try to get a sense of the big picture. In particular, don’t worry about the Python codes on the first read.</p>
<p>The usual setup in pattern recognition problems is that of supervised learning (the two terms are all but synonymouos, minus the culture of “object recognition”).</p>
<p>The train–test split is a hallmark of supervised learning.
While we are designing our pattern recognition system we can do whatever we want with the training set but we are not allowed to touch the test set. Once the system is designed and trained we apply it to the test set to get a sense of how it’ll perform on new inputs. Of course, we assume that these new inputs will somehow resemble the ones used for training.</p>
</section>
<section id="let-s-focus-on-two-digits-binary-classification">
<h2>Let’s focus on two digits: binary classification<a class="headerlink" href="#let-s-focus-on-two-digits-binary-classification" title="Permalink to this headline">#</a></h2>
<p>To avoid needless complexity, we will first try to build a <em>binary</em> classifier which distinguishes between two classes of handwritten digits. For example, it should recognize whether the input digit is a “4” or a “7”. We begin by writing some spaghetti code to extract a random sample which only contains the chosen two digits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>classes = [4, 7]
d = 28**2
X = np.ndarray((0, d))
y = np.ndarray((0, 1))

for cl in classes:
    X = np.vstack((X, X_train[y_train == cl]))
    y = np.vstack((y, y_train[y_train == cl, np.newaxis]))

n = 5000
shuffle_idx = np.random.choice(len(y), n)
X = X[shuffle_idx]
y = y[shuffle_idx].flatten()
y = 2*(y == classes[0]) - 1
</pre></div>
</div>
</div>
</div>
<p>We quickly verify that we indeed only have the two chosen digits by showing a small random sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, axs = plt.subplots(ncols=4, nrows=1, figsize=(14, 3))
for ax in axs:
    ax.imshow(X[np.random.randint(1000)].reshape(28, 28), cmap=&#39;gray&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="rule-0-simple-ideas-first">
<h2>Rule 0: simple ideas first<a class="headerlink" href="#rule-0-simple-ideas-first" title="Permalink to this headline">#</a></h2>
<p>We begin with a very simple idea (although perhaps the second one will be even simpler… ymmv). In mathematics and machine learning, <em>simple</em> often means <em>linear</em>, so we start with a linear classifier. What do we mean by linear?</p>
<p>Classifiers output discrete labels. In our case the label is the digit, but since we’re only looking at two digits we can use labels +1 and -1. The usual way to obtain these labels is to apply a function to a digit and then take the sign of the output,</p>
<div class="math notranslate nohighlight">
\[
	\hat{y} = \mathrm{sign}(f(\text{digit}; \theta)).
\]</div>
<p>We use a generic symbol <span class="math notranslate nohighlight">\(\theta\)</span> to indicate our function has some parameters. The whole point of “learning” or “training” is to determine the parameters which yield decent classification. The hat on <span class="math notranslate nohighlight">\(\hat{y}\)</span> indicates an <em>estimate</em> or a <em>prediction</em> we are making about the true label <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>MNIST digits are 8-bit grayscale images of size <span class="math notranslate nohighlight">\(28 \times 28\)</span>. Denoting a digit by <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can fancily write</p>
<div class="math notranslate nohighlight">
\[
	\mathbf{x} \in \{0, 1, \ldots, 255\}^{28 \times 28}.
\]</div>
<p>Instead of working with a square grid of pixels as above, we will instead work with its vectorized version obtain by stacking all rows or all columns into a long vector of length <span class="math notranslate nohighlight">\(28^2 = 784\)</span>. The choice of rows or columns (or any other scheme) is irrelevant as long as we consistently stick to it. We will sometimes denote a vectorization of a matrix by <span class="math notranslate nohighlight">\(\mathrm{vec}(\mathbf{x})\)</span>, but here we will abuse notation (abusing notation is math–academic speak) and simply again write <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{x} = 
    \begin{bmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_d,
    \end{bmatrix},
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(d = 784\)</span>.</p>
<p>We said that we will build a linear classifier. This means that <span class="math notranslate nohighlight">\(f\)</span> is a linear function:</p>
<div class="math notranslate nohighlight">
\[
	f(\alpha \mathbf{x}_1 + \beta \mathbf{x}_2) = \alpha f(\mathbf{x}_1) + \beta f(\mathbf{x}_2).
\]</div>
<p>It can be checked that all scalar-valued linear functions of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are given as dot products with some vector of weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. Thus our classifier returns the label</p>
<div class="math notranslate nohighlight">
\[
	\hat{y} = \mathrm{sign}\left(\sum_{i = 1}^d w_i x_i\right) = \mathrm{sign}(\mathbf{w} \cdot \mathbf{x}) = \mathrm{sign}(\langle \mathbf{w}, \mathbf{x} \rangle) = \mathrm{sign}(\mathbf{w}^T \mathbf{x}).
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\theta = \mathbf{w}\)</span>.</p>
</section>
<section id="choosing-the-right-weights">
<h2>Choosing the right weights<a class="headerlink" href="#choosing-the-right-weights" title="Permalink to this headline">#</a></h2>
<p>We now need to decide how to choose the best weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. In order to do that we have to clearly state what we mean by <em>best</em>, but before doing that let us try something goofy 🤭: random weights. More concretely let us independently sample each <span class="math notranslate nohighlight">\(w_i\)</span> from a standard normal distribution. (Standard means zero mean and unit variance).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># generate random weights
w_random = np.random.randn(d)

# predict labels
y_est = np.sign(X @ w_random)

correct = np.sum(y_est * y == 1)
print(&#39;%d out of %d patterns correctly classified (%5.2f%%)&#39; % (correct, n, 100*correct/n))
</pre></div>
</div>
</div>
</div>
<p>Not very surprisingly, a linear classifier with random weights does not achieve a particularly impressive digit classification performance. On the other hand, we can see that there the values of <span class="math notranslate nohighlight">\(\mathbf{w}_{\text{random}}^T \mathbf{x}\)</span> vary wildly—some are very small and thus “almost” correctly classified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(7, 5))
ax.hist((X @ w_random)[y_est * y == -1], bins=20)
ax.set_title(&#39;$\mathbf{w}_{\mathrm{random}}^T \mathbf{x}$ for misclassified patterns&#39;);
</pre></div>
</div>
</div>
</div>
<p>A thought that naturally comes to mind is: can we somehow perturb the weights so that some of those patterns that are almost correctly classified pass to the correct class. Before figuring out how to do this visually, by geometry, we can think about the problem purely algebraically. Suppose that some training pattern <span class="math notranslate nohighlight">\(\mathbf{x}_{-1}\)</span> whose label is <span class="math notranslate nohighlight">\(-1\)</span> is misclassified; that is, <span class="math notranslate nohighlight">\(\mathbf{w}_{\mathrm{random}}^T \mathbf{x}_{-1} &gt; 0\)</span>. A simple way to change the weights so that this pattern becomes correctly classified is to subtract from <span class="math notranslate nohighlight">\(\mathbf{w}_{\mathrm{rnd}}\)</span> some multiple of <span class="math notranslate nohighlight">\(\mathbf{x}_{-1}\)</span> since that will decrease the dot product and (for a sufficiently large multiplicative factor) make it negative, which is what we want. Indeed, letting <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[
    (\mathbf{w}_{\mathrm{rnd}} - \alpha \mathbf{x}_{-1})^T \mathbf{x}_{-1}
    =
    \mathbf{w}_{\mathrm{rnd}}^T \mathbf{x}_{-1} - \alpha \| \mathbf{x}_{-1} \|^2  
    &lt; 
    \mathbf{w}_{\mathrm{rnd}}^T \mathbf{x}_{-1}.
\]</div>
<p>For <span class="math notranslate nohighlight">\(\alpha\)</span> big enough this quantity will become negative. Similarly, if a pattern <span class="math notranslate nohighlight">\(\mathbf{x}_{+1}\)</span> with label <span class="math notranslate nohighlight">\(+1\)</span> is misclassified so that <span class="math notranslate nohighlight">\(\mathbf{w}_{\mathrm{random}}^T \mathbf{x}_{+1} &lt; 0\)</span>, then we could <em>add</em> a multiple of <span class="math notranslate nohighlight">\(\mathbf{x}_{+1}\)</span> to make the inner product positive. For <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
    (\mathbf{w}_{\mathrm{rnd}} + \alpha \, \mathbf{x}_{+1})^T \mathbf{x}_{+1}
    =
    \mathbf{w}_{\mathrm{rnd}}^T \mathbf{x}_{+1} + \alpha \| \mathbf{x}_{+1} \|^2  
    &gt; 
    \mathbf{w}_{\mathrm{rnd}}^T \mathbf{x}_{+1}.
\]</div>
<p>For <span class="math notranslate nohighlight">\(\alpha\)</span> big enough this quantity will become positive.</p>
<p>We can visualize this by pretending that we work with “digits” that only have 2 pixels (instead of <span class="math notranslate nohighlight">\(28 \times 28 = 784\)</span>). The usefulness of this simplification is that now every “digit” can be identified with a point in the plane with coordinates <span class="math notranslate nohighlight">\((x, y) = (\textsf{pixel 1}, \textsf{pixel 2})\)</span>. Consider the situation in the left subfigure below. The weight vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> defines a line which splits the space into a positive and a negative halfspace. All points in the upper halfspace which is in the direction of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> form accute angles (<span class="math notranslate nohighlight">\(&lt; 90^\circ\)</span>) with <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and thus have a positive inner product with <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> (because the cosine of an accute angle is positive). Similarly, all points in the lower halfspace form obtuse angles with <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and thus have negative inner products with <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. If blue squares are the positive class (label +1) and red circles the negative class (label -1), then we see from this picture that the current classifier misclassifies three training patterns.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>before</p></th>
<th class="head"><p>after</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="fig2Da" src="_images/linear-random-1.svg" /></p></td>
<td><p><img alt="fig2Db" src="_images/linear-random.svg" /></p></td>
</tr>
</tbody>
</table>
<p>Let us try to perturb (rotate) <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> so that the misclassified red circle closest to the boundary becomes correctly classified. Let’s call this pattern <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span>. We have that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^T \tilde{\mathbf{x}} &gt; 0 
\]</div>
<p>but it should be negative. Per above discussion, we want to compute a perturbation in the direction of negative <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span>, say <span class="math notranslate nohighlight">\(- \alpha \tilde{\mathbf{x}}\)</span> so that</p>
<div class="math notranslate nohighlight">
\[
    (\mathbf{w} - \alpha \tilde{\mathbf{x}})^T \tilde{\mathbf{x}} = 0
\]</div>
<p>which will place <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> exactly on the decision boundary (exactly on the line). Thus</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{w}^T \tilde{\mathbf{x}} - \alpha \tilde{\mathbf{x}}^T \tilde{\mathbf{x}} = 0 
    \Longrightarrow
    \alpha = \frac{\mathbf{w}^T \tilde{\mathbf{x}}}{\tilde{\mathbf{x}}^T \tilde{\mathbf{x}}}. 
\]</div>
<p>Let’s now apply these ideas to digit classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># find indices of misclassfied examples
idx_miss = np.where(y != y_est)[0]

idx_miss
i = idx_miss[np.argmin(np.abs((X @ w_random)[idx_miss]))]
dot_i = np.dot(X[i], w_random)
print(&#39;The smallest absolute dot product is %.2e&#39; % (dot_i,))
</pre></div>
</div>
</div>
</div>
<p>We identified the pattern (a digit) which is the closest to the linear decision boundary in the sense of angle). Note that this does not mean that it is the closest in distance.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Why?</p>
<p>The dot product <span class="math notranslate nohighlight">\(\mathbf{a}^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \angle(\mathbf{a}, \mathbf{b})\)</span>. Thus if the digit in question has a particularly large Euclidean norm (it is far from the origin), the angle with the boundary could be small but the distance to the boundary still large.</p>
</div>
<p>Now we add the computed perturbation to the weights, but we scale it by a number slightly larger than 1 so that the digit does not land on the decision boundary but (barely) crosses to the other side.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>w_bumped = w_random - 1.000001 * X[i] * dot_i \
                / np.linalg.norm(X[i])**2
y_est_bumped = np.sign(X @ w_bumped)
correct_bumped = np.sum(y_est_bumped * y == 1)
print(&#39;Correct before: %d/%d, correct after: %d/%d&#39; % (correct, n, correct_bumped, n))
</pre></div>
</div>
</div>
</div>
<p>We see that the new weights indeed correctly classify one additional digit which was previously misclassified. The reason to multiply by 1.000001 instead of, for example, by 2, is that in this latter case we would likely mess things up for some digits that used to be classified correctly (as well as correct things for <em>other</em> digits that used to be misclassified). A simple count would then not be sufficient to check whether the ideas work.</p>
</section>
<section id="let-s-make-this-into-an-algorithm">
<h2>Let’s make this into an algorithm<a class="headerlink" href="#let-s-make-this-into-an-algorithm" title="Permalink to this headline">#</a></h2>
<p>A question comes to mind: even if we mess things up for some other digits, we could then apply our perturbation ideas to them, and cycle over misclassified digits until, hopefully, most of them become correctly classified. As we will see, this is not always possible (the patterns should be <em>linearly separable</em> for it to be possible; in the above figure, there should exist a line which perfectly separates squares and circles). But it can often work quite well. Our strategy is as follows: for each misclassified pattern we will update the weights in the direction which would make the pattern correctly classified. A parameter called <em>learning rate</em> will determine how strongly we bump the weights in that direction. Once we go through all training patterns we will start from the beginning, and repeat this process until the estimated classes (signs of dot products) stabilize, or, simpler, for some set number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train(X, y, lr=0.1, n_iter=30):
    # X : a (n, d) matrix with rows being training examples (digits)
    # y : a (d, 1) vector of labels

    n, d = X.shape
    w = np.zeros((d,))
    
    for i_iter in range(n_iter):
        for i_example in range(n):
            y_est = np.sign(np.dot(X[i_example], w))

            if y_est != y[i_example]:
                w += lr * X[i_example] * y[i_example]
        
    return w
</pre></div>
</div>
</div>
</div>
<p>(Check class notes to see a better implementation.) We now apply this to MNIST.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>w = train(X, y)

error_count = np.sum(np.sign(X @ w) * y &lt; 0 )
print(&quot;%d out of %d patterns are misclassified&quot; % (error_count, n))
</pre></div>
</div>
</div>
</div>
<p>A much better result than what we initially obtained with random weights! Let’s try to visualize the digits in a way that clearly shows that we’ve got ourselves some good weights. We need a way to represent the digits which live in a <span class="math notranslate nohighlight">\(28 \times = 784\)</span> dimensional space by points in the 2D plane. We will do it like this: choose two directions <span class="math notranslate nohighlight">\(\mathbf{e}_1, \mathbf{e}_2 \in \mathbb{R}^{784}\)</span> and then represent a digit <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> by a point <span class="math notranslate nohighlight">\((\mathbf{e}_1^T \mathbf{x}, \mathbf{e}_2^T \mathbf{x})\)</span> in the plane. We’ll choose <span class="math notranslate nohighlight">\(\mathbf{e}_2 = \mathbf{w} / \| \mathbf{w} \|\)</span> becase we’re interested in what the distribution of points looks like along the direction of the weights. Then we’ll set <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span> to some unit vector orthogonal to <span class="math notranslate nohighlight">\(\mathbf{e}_2\)</span>. Since there are 783 orthogonal directions, we choose one at random. You can check that the computed vectors are indeed orthogonal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>w_orth = np.random.randn(d) 
w_orth -= w * np.dot(w_orth, w) / np.linalg.norm(w)**2

X_w = X @ w / np.linalg.norm(w)
X_orth = X @ w_orth / np.linalg.norm(w_orth)

fig, ax = plt.subplots(figsize=(8, 5))
ax.scatter(X_orth[y==1], X_w[y==1])
ax.scatter(X_orth[y==-1], X_w[y==-1]);
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-do-the-weights-do">
<h2>What do the weights do?<a class="headerlink" href="#what-do-the-weights-do" title="Permalink to this headline">#</a></h2>
<p>An important benefit of using simple linear classifiers is that we can visualize the weights as a 28 <span class="math notranslate nohighlight">\(\times\)</span> 28 image and see exactly how the different digits are distinguished. Such <em>interpretability</em> is a desirable property of pattern recognition systems, but it’s often absent in modern approaches based on large deep neural networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(figsize=(8, 8))
im = ax.imshow(w.reshape(28, 28), cmap=&#39;winter&#39;)
_ = fig.colorbar(im, ax=ax, shrink=0.6)
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-different-classifier-counting-neighbor-labels">
<h2>A different classifier: counting neighbor labels<a class="headerlink" href="#a-different-classifier-counting-neighbor-labels" title="Permalink to this headline">#</a></h2>
<p>We have successfully constructed our first pattern classification system. The idea was perhaps a bit abstract and probably different from how you would go about classifying patterns by hand.</p>
<figure class="align-default" id="id2">
<img alt="_images/Meyer_Lemon.jpg" src="_images/Meyer_Lemon.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">A Meyer lemon.</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Imagine I ask you to classify a citrus as either a Meyer lemon or an ordinary lemon. I also give you 30 ordinary lemons and 30 Meyer lemons. The way you’d solve this problem is most likely by comparing the lemon to the two piles and assigning it to the pile that contains more similar lemons.</p>
<p>Let’s turn this intuitive approach into an algorithm. The idea is as follows: for a patter we want to classify, we will find <span class="math notranslate nohighlight">\(k\)</span> patterns that are closest to it in the training set. Then we will do a majority vote: find the label that occurs most often among the neighbors and declare this label to be our estimate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>k = 31
error_count = 0
for i_example in range(n):

    # compute distance from the current example
    # to all other
    dist2 = np.sum((X - X[i_example])**2, axis=1)

    # find k closest examples, ignoring zero 
    # distance to the current one
    k_closest_idx = np.argsort(dist2)[1:k + 1]

    # do majority vote on the corresponding labels
    y_est = np.sign(y[k_closest_idx].sum())

    # check for errors
    if y_est * y[i_example] &lt; 0:
        error_count += 1

print(&quot;The number of misclassified points is: &quot;, error_count)
</pre></div>
</div>
</div>
</div>
<p>This is pretty slow but in the end we get quite a decent result. The right thing to do would be to test how this works for patterns outside of the training set, but for the moment let’s just keep playing around and building intuition.</p>
</section>
<section id="which-k-works-best">
<h2>Which <span class="math notranslate nohighlight">\(k\)</span> works best?<a class="headerlink" href="#which-k-works-best" title="Permalink to this headline">#</a></h2>
<p>In the above example we chose the number of neighbors as <span class="math notranslate nohighlight">\(k = 31\)</span> for no particular reason. Intutively, this value should have a marked effect on the behavior of our classifier. Let us test many different values of <span class="math notranslate nohighlight">\(k\)</span> to figure out which one is the best. Before we do that… we need to do something with the code above which is far too slow to run over and over for different <span class="math notranslate nohighlight">\(k\)</span>. The idea is to leverage fast mathematical primitives used by <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, in particular matrix multiplication. To understand what the formula in the below code does, check out this <a class="reference external" href="https://infoscience.epfl.ch/record/221380/files/EDM%283%29.pdf">paper about Euclidean distance matrices</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ec = []
G = X @ X.T # ref: EDMs -&gt; hw
dist22 = np.diag(G)[:, np.newaxis] - 2*G + np.diag(G)[np.newaxis, :]
sorted_dist_idx = np.argsort(dist22, axis=1)

for k in range(2, 100, 2):
    k_closest_idx = sorted_dist_idx[:, 1:k + 1]
    y_est = np.sign(y[k_closest_idx].sum(axis=1))
    error_count = (y_est * y &lt;= 0).sum()    
    ec.append(error_count)

plt.plot(ec, linewidth=3)
plt.xlabel(&#39;$k$&#39;)
plt.ylabel(&#39;Misclassified points&#39;);
</pre></div>
</div>
</div>
</div>
<p>It seems that both <span class="math notranslate nohighlight">\(k\)</span> too small and <span class="math notranslate nohighlight">\(k\)</span> too large are bad. Can you guess why?</p>
<p>It would be great to get some visual intution about what is going on, but we cannot easily do that for the <span class="math notranslate nohighlight">\(28 \times 28\)</span> digits: we would have to find a way to visualize 784-dimensional space. Luckily there are many datasets with a small number of features which make more sense.</p>
</section>
<section id="the-iris-flower-datset">
<h2>The Iris flower datset<a class="headerlink" href="#the-iris-flower-datset" title="Permalink to this headline">#</a></h2>
<p>Another legendary dataset used by almost every data science course on the planet is the Iris flower dataset compiled by Ronald Fisher, probably the most important statistician of all time (and a complicated person privately and publicly).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Modified from Gaël Varoquaux

from sklearn import datasets

iris = datasets.load_iris()
X_iris = iris.data[:, :2]  # take only the first two features
y_iris = iris.target

x_min, x_max = X_iris[:, 0].min() - 0.5, X_iris[:, 0].max() + 0.5
y_min, y_max = X_iris[:, 1].min() - 0.5, X_iris[:, 1].max() + 0.5

plt.figure(figsize=(7, 5))

plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap=plt.cm.Set1, edgecolor=&quot;k&quot;)
plt.xlabel(&quot;Sepal length&quot;)
plt.ylabel(&quot;Sepal width&quot;)

plt.xlim(x_min, x_max)
_ = plt.ylim(y_min, y_max)
</pre></div>
</div>
</div>
</div>
<p>We again focus on two classes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>excluded_class = 2
classes = np.setdiff1d([0, 1, 2], excluded_class)
X = X_iris[y_iris != excluded_class]
y = y_iris[y_iris != excluded_class]
y[y == classes[0]] = -1
y[y == classes[1]] = 1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X -= X.mean(axis=0)

w = train(X, y)
w /= np.linalg.norm(w)
w_orth = np.array([1, -w[0] / w[1]])

x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

plt.figure(figsize=(6, 4))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=&quot;k&quot;)
s = 20
plt.plot([-s*w_orth[0], s*w_orth[0]], [-s*w_orth[1], s*w_orth[1]], &#39;k&#39;, linewidth=2)
plt.xlabel(&quot;Sepal length&quot;)
plt.ylabel(&quot;Sepal width&quot;)
plt.xlim(x_min, x_max)
_ = plt.ylim(y_min, y_max)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>k = 51
n_points = 200
x_ = np.linspace(x_min, x_max, n_points)
y_ = np.linspace(y_min, y_max, n_points)

xx, yy = np.meshgrid(x_, y_)
label = np.zeros(xx.shape)

for i in range(n_points):
    for j in range(n_points):
        point = np.array([xx[i, j], yy[i, j]]) # classify current point
        knn_idx = np.argsort(np.sum((X - point)**2, axis=1))[:k]
        label[i, j] = np.sign(y[knn_idx].sum())

plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=(y/2+1), cmap=plt.cm.Set1, edgecolor=&quot;k&quot;)
plt.plot([-s*w_orth[0], s*w_orth[0]], [-s*w_orth[1], s*w_orth[1]], &#39;k&#39;, linewidth=2)
_ = plt.imshow(np.flipud(label), cmap=&#39;summer&#39;, extent=[x_min, x_max, y_min, y_max])
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-perceptron">
<h2>The perceptron<a class="headerlink" href="#the-perceptron" title="Permalink to this headline">#</a></h2>
</section>
<section id="well-we-first-reinvented-the-perceptron">
<h2>Well, we first reinvented the <em>perceptron</em><a class="headerlink" href="#well-we-first-reinvented-the-perceptron" title="Permalink to this headline">#</a></h2>
<figure class="align-right">
<a class="reference internal image-reference" href="_images/rosenblatt.jpg"><img alt="_images/rosenblatt.jpg" src="_images/rosenblatt.jpg" style="width: 220.0px; height: 286.0px;" /></a>
</figure>
<p>The perceptron is a brainchild of Frank Rosenblatt who was an American psychologist working on artificial intelligence. The algorithm received major press coverage at the time, with the hype not too different from what we have witnessed over the past decade with deep learning (with the notable difference that the perceptron was indeed quite limited in its capabilities and generalizations were far away). Frank Rosenblatt’s discovery has huge implications today since the perceptron is more or less the artificial neuron used in today’s deep neural networks, and the training algorithm (which is really the key) is related to error backpropagation used to train modern deep neural networks.</p>
<figure class="align-left" id="id3">
<a class="reference internal image-reference" href="_images/nytp.png"><img alt="_images/nytp.png" src="_images/nytp.png" style="width: 721.0px; height: 462.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">The New York Times, 1958</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="id4">
<a class="reference internal image-reference" href="_images/0925_rosenblatt4.jpg"><img alt="_images/0925_rosenblatt4.jpg" src="_images/0925_rosenblatt4.jpg" style="width: 600.0px; height: 357.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">The New York Times, 1958</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="we-then-reinvented-k-nearest-neighbors">
<h2>We then reinvented <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors<a class="headerlink" href="#we-then-reinvented-k-nearest-neighbors" title="Permalink to this headline">#</a></h2>
<figure class="align-left">
<a class="reference internal image-reference" href="_images/tomcover.jpg"><img alt="_images/tomcover.jpg" src="_images/tomcover.jpg" style="width: 100.0px; height: 150.0px;" /></a>
</figure>
<ul class="simple">
<li><p>Evelyn Fix &amp; Joseph Lawson Hodges Jr. circa 1951</p></li>
<li><p>Expanded and analyzed by Thomas Cover (on the left)</p></li>
<li><p>Despite simplicity it is an incredibly useful technique, and one which has been keeping machine learning theorists busy for many years</p></li>
</ul>
</section>
<section id="some-of-the-questions-we-must-answer">
<h2>Some of the questions we must answer<a class="headerlink" href="#some-of-the-questions-we-must-answer" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Which classifier is better?</p></li>
<li><p>What does “better” even mean?</p></li>
</ul>
</section>
<section id="we-have-been-committing-a-crime">
<h2>We have been committing a crime<a class="headerlink" href="#we-have-been-committing-a-crime" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>We “trained”, “validated”, and “tested” on one and the same dataset</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k\)</span>NN it’s even a bit silly (consider the iris example)</p></li>
</ul>
</section>
<section id="this-course">
<h2>This course<a class="headerlink" href="#this-course" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>“Pattern recognition” very close to “machine learning”</p></li>
<li><p>Focus in PR is on “supervised learning”</p>
<ul>
<li><p>classification</p></li>
<li><p>prediction</p></li>
</ul>
</li>
<li><p>But “unsupervised learning” plays a key role</p>
<ul>
<li><p>representation learning</p></li>
<li><p>feature maps: learning good features</p></li>
</ul>
</li>
</ul>
</section>
<section id="beware">
<h2>Beware<a class="headerlink" href="#beware" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>The language of pattern recognition is mathematical</p></li>
<li><p>We will give intuition but to really get it, make sure you know your probability and linear algebra well</p></li>
<li><p>That is a good thing; without the formal understanding PR would not be where it’s now</p></li>
<li><p>Let me tell you a story about interviewing people for CeDA…</p></li>
</ul>
</section>
<section id="key-mathematical-tools">
<h2>Key mathematical tools<a class="headerlink" href="#key-mathematical-tools" title="Permalink to this headline">#</a></h2>
<section id="probability-theory-and-statistics">
<h3>Probability theory (and statistics)<a class="headerlink" href="#probability-theory-and-statistics" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>What is a random variable? A random vector?</p></li>
<li><p>A joint distribution? A conditional distribution? An expectation?</p></li>
<li><p>What are independent random variables?</p></li>
<li><p>…</p></li>
</ul>
</section>
<section id="linear-algebra">
<h3>Linear algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>What does <span class="math notranslate nohighlight">\(\mathbf{w}^T \mathbf{x}\)</span> mean? How about <span class="math notranslate nohighlight">\(\| \mathbf{x} \|\)</span>?</p></li>
<li><p>Why is <span class="math notranslate nohighlight">\(\arg \min_{\mathbf{w}} ~ \| \mathbf{y} - \mathbf{X} \mathbf{w} \|^2 + \lambda \| \mathbf{x} \|^2 \)</span> precisely <span class="math notranslate nohighlight">\(\mathbf{x}^* = ( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}\)</span></p></li>
<li><p>What does it mean to invert a matrix?</p></li>
<li><p>What is a projection?</p></li>
<li><p>…</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="installation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Installation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="modeling-knowledge.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Modeling Knowledge</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ivan Dokmanić<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>